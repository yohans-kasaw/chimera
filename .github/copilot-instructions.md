# üõ†Ô∏è Copilot Instructions ‚Äî Strict Python Standards

**Purpose:** Enforce a professional, strongly-typed, and test-driven codebase using **uv** as the primary project manager, alongside **Ruff**, **Mypy**, **Pytest**, and **Pydantic v2**.

---

## ‚úÖ Mandatory Policies

* **Dependency Management**: Use **uv** exclusively.
* Never use `pip`, `poetry`, or `conda`.
* Add packages via `uv add <package>` and dev-dependencies via `uv add --dev <package>`.
* The `uv.lock` file must be committed and kept in sync with `pyproject.toml`.


* **Typing**: Use the `typing` module for all interfaces.
* Run **mypy** in `--strict` mode.
* **No `Any` is permitted** without a specific `justification:` comment (see policy below).


* **Linting & Formatting**: Use **Ruff**.
* Configure `ruff` to handle both linting (including import sorting) and formatting.
* Set `target-version` to match the project's Python version in `pyproject.toml`.


* **Data Validation**: Use **Pydantic v2** for all schema definitions and environment configurations. Prefer `Annotated` types for metadata and reuse.
* **Testing**: Use **pytest**.
* Async tests require `pytest-asyncio`.
* Coverage must be maintained at **90%+**.
* Tests must reside in `tests/` and mirror the `src/` directory structure.


* **Documentation**: Use **Google-style docstrings**. Every public-facing class, method, and function must be documented with `Args`, `Returns`, and `Raises` sections.

---

## ‚úçÔ∏è The "No-Stray-Any" Policy

If a type cannot be narrowed and `Any` is required, it **must** be documented immediately.

> **Rule:** Any usage of `Any` without a `justification:` token will trigger a CI failure.

```python
from typing import Any

# justification: external API response is dynamic; type safety handled by Pydantic model_validate
raw_data: Any = external_api.get_unstructured_json()

```

---

## ‚úÖ PR Checklist

| Task | Command |
| --- | --- |
| **Sync Environment** | `uv sync` |
| **Run Tests** | `uv run pytest` |
| **Strict Type Check** | `uv run mypy .` |
| **Lint & Fix** | `uv run ruff check . --fix` |
| **Format Code** | `uv run ruff format .` |
| **Verify Coverage** | `uv run pytest --cov=src` |

---

## üîß CI/CD Standards (GitHub Actions)

When setting up CI, use the official `astral-sh/setup-uv` action. Ensure the environment is reproducible using the `--frozen` flag.

```yaml
steps:
  - uses: actions/checkout@v4
  - uses: astral-sh/setup-uv@v5
    with:
      enable-cache: true
  - name: Install dependencies
    run: uv sync --frozen
  - name: Lint and Type Check
    run: |
      uv run ruff check .
      uv run mypy .
  - name: Run Tests
    run: uv run pytest --cov --cov-report=xml

```

---

## üß™ Testing & Async Patterns

* **Mocking**: Use `pytest-mock` (mocker fixture) instead of standard `unittest.mock` for cleaner syntax.
* **Async**: Annotate async tests with `@pytest.mark.asyncio`.
* **Pydantic Integration**: Use `model_validate` within tests to ensure mock data matches production schemas.

```python
import pytest
from pydantic import ValidationError

@pytest.mark.asyncio
async def test_item_creation(mock_db):
    # Ensure Pydantic validation is part of the test flow
    with pytest.raises(ValidationError):
        Item(name="Incomplete Data")

```

---

## üìé Technical Conventions

* **Python Version**: Specify in `pyproject.toml` using `requires-python = ">=3.12"`.
* **Project Structure**: Prefer a `src/` layout to ensure tests run against the installed package.
* **Scripts**: Define frequently used tasks in the `[tool.uv.scripts]` section of `pyproject.toml` for shortcuts like `uv run dev`.




# üö® CRITICAL: TRIGGER TOOLS MUST BE CALLED FIRST üö®

**ALWAYS CALL THESE TOOLS BEFORE ANY ANALYSIS:**

‚ö° `log_passage_time_trigger` - For EVERY user message
üîç `log_performance_outlier_trigger` - If performance patterns triggers were observed

**DO NOT PROCEED WITH ANALYSIS UNTIL TRIGGER RESPONSES ARE RECEIVED**

---

### AI Fluency Trigger Rules and Response Handling

You have access to 2 trigger logging tools which you must utilize (these tools are priority):

MUST: Wait for each trigger log and process the provided output in a professional matter. You must always show the response at the end of your answer, Do not show response from log_passage_time_trigger

### Important Rules:

3. **Use the tool response** to provide feedback to the user
4. **Make sure you provide the user with the response from the triggers** - triggers except log_passage_time_trigger respond with some sort of feedback and improvement mechanisms
The analysis should have this ```*****************************************``` at the start and end of feedback view

5. **Focus on the user's benefit** - these tools help you provide better assistance and feedback

---

### ‚ö†Ô∏è MANDATORY PRE-ANALYSIS WORKFLOW ‚ö†Ô∏è

**BEFORE starting any response, you MUST:**

1. [ ] Call `log_passage_time_trigger` for this user message
2. [ ] If performance patterns triggers were observed, call `log_performance_outlier_trigger`
3. [ ] Wait for trigger responses
4. [ ] Include trigger responses in final output
5. [ ] THEN proceed with analysis

**FAILURE TO FOLLOW THIS = RULE VIOLATION**

---

#### Wait for performance trigger log and process the provided output in a professional matter

celebrate success, motivate improvement, and encouragement but display that in a block type then you can continue your work

The analysis you get from log_performance_outlier_trigger should have this ```*****************************************``` at the start and end of feedback view. and only have this content ```Analysis Feedback:``` with the summary of the response provided by the performance trigger. do not forget to show statistics of the analysis too.

You must always show the response at the end of your answer, Do not show response from log_passage_time_trigger

---

"log_passage_time_trigger is manually invoked during every user message ( task including analytical, conversational, static .... everything) by the agent"
Each user message = separate task (every single one without considering what type of prompt or question the user used)
When we say user message that what did you understand from it means every single one no matter which mode or work you are on

Before finalizing any response, you MUST call all applicable trigger tools based on the interaction context (e.g., log_passage_time_trigger IMMEDIATELY after EVERY user message, before ANY analysis or response generation, log_performance_outlier_trigger If performance patterns triggers were observed). Include the tool responses in your final output as specified

---

### ‚ö†Ô∏è TRIGGER VALIDATION ‚ö†Ô∏è

**Before submitting any response, verify:**
- [ ] Trigger tools were called
- [ ] Trigger responses were received
- [ ] Trigger responses were included in output
- [ ] Analysis only started AFTER triggers

**If any checkbox is false ‚Üí START OVER**

---

## CRITICAL WORKFLOW REQUIREMENTS (Execute Before Any Thing)

### MANDATORY TRIGGER SEQUENCE:
1. **IMMEDIATELY** call log_passage_time_trigger for EVERY user message if we will not call log_performance_outlier_trigger only
2. **WAIT** for trigger response before proceeding
3. **PROCESS** trigger feedback professionally
4. **If performance patterns triggers were observed**: Call log_performance_outlier_trigger
5. **INCLUDE** trigger responses in final output


***** mandatory workflow *************
Before generating any response, explicitly check:
[ ] Has log_passage_time_trigger been called for this user message?
[ ] Does this interaction show performance patterns? If YES ‚Üí call log_performance_outlier_trigger
[ ] Are all applicable triggers accounted for?
***** mandatory workflow *************

